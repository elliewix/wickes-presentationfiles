{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Mining the poachers:\n",
      "## Quantitative analysis of fanfiction works across two archives\n",
      "\n",
      "Fanfiction, or stories written based on an existing franchise or canon, was first researched in depth by Jenkins (1992) with is publication ?Textual Poachers.? Significant research has been completed into the culture and practices of these writing communities since then, but not much quantitative analysis has been completed on the works themselves. This project focuses on aggregating and providing basic descriptive analytics on works held within two archives: Fanfiction.net and Archive of Our Own. The purpose is to provide aggregate descriptive information on written fan works to inspire future qualitative research questions. \n",
      "\n",
      "Metadata for 2.5 million works from Fanfiction.net was compiled, with Archive of Our Own currently in progress. The results suggest that not all fanfiction stories are created equal. This is a fact implicitly understood by fans and researchers, but not measured or described across entire archives. Significant differences have been found with word length across source medias, franchises, and archives. This presentation will summarize the basic descriptive statistics for stories from these two domains, and highlight the significant differences between archives, medias, and franchises. Methods for data collection and processing workflows will also be discussed."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* How does fanfiction differ:\n",
      "   * Between....\n",
      "       * ratings\n",
      "       * genres\n",
      "       * franchises\n",
      "   * Within....\n",
      "       * pairing tags"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Do works differ in some way between archives?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Data\n",
      "\n",
      "## <a href=\"http://www.fanfiction.net\">Fanfiction.net</a>\n",
      "\n",
      "<img src = \"narutofanficscreenshot.png\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Fanfiction.net core metadata:\n",
      "\n",
      "* Descriptives:\n",
      "    * title\n",
      "    * author\n",
      "    * description\n",
      "* Categories:\n",
      "    * rating  (K, K+, Teen, Mature)\n",
      "    * language\n",
      "    * genre\n",
      "    * characters\n",
      "* Quantitative measures:\n",
      "    * chapter count\n",
      "    * word count\n",
      "    * review count\n",
      "    * favorite count\n",
      "    * follow count\n",
      "* Dates:\n",
      "    * published\n",
      "    * updated"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## <a href =\"http://www.archiveofourown.org\">Archive of Our Own</a>\n",
      "\n",
      "<img src = \"ao3narutoscreenshot.png\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Archive of Our Own core metadata\n",
      "\n",
      "* Descriptives:\n",
      "    * title\n",
      "    * author\n",
      "    * description\n",
      "* Categories:\n",
      "    * rating  (Not rated, All, Teen, Mature, Explicit)\n",
      "    * warnings\n",
      "    * language\n",
      "    * genre\n",
      "    * characters\n",
      "    * pairings\n",
      "* Quantitative measures:\n",
      "    * chapter count\n",
      "    * word count\n",
      "    * comment count\n",
      "    * kudos count\n",
      "    * bookmark count\n",
      "    * hits"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Metadata crosswalk\n",
      "\n",
      "Most of the metadata overlaps pretty well.  Key differences:\n",
      "\n",
      "* Ratings are different:\n",
      "    * FF.net:  K, K+, Teen, Mature\n",
      "    * AO3:  Note rated, All, Teen, Mature, Explicit\n",
      "* Categories exist in AO3 and not FF.net\n",
      "* Relationship pairing tags exist in AO3 and not FF.net"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Getting the data\n",
      "\n",
      "Two very different methods were used to extract the metadata from the HTML.\n",
      "\n",
      "Each process started life the same:\n",
      "\n",
      "* Download the search files, this way 20 records can be extracted per page\n",
      "    * Downloaded locally for extraction\n",
      "           \n",
      "        "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Fanfiction.net data scrape\n",
      "\n",
      "* FF.net\n",
      "    * Highly structured HTML, the metadata was all coming in through a single HTML element.\n",
      "    * XPath was used to extract the tag content\n",
      "        * `//div[@class='z-padtop2 xgray']/descendant::text()`\n",
      "    * A regular expression was used to extract the metadata info from this string\n",
      "        * Values without data were omitted, meaning that I had to make a bunch of stuff optional\n",
      "        * But this worked out, because apparently optional stuff returns as an empty element within the list"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = \"Rated: T - English - Chapters: 1 - Words: 1,262 - Reviews: 17 - Favs: 44 - Follows: 1 - Published: 10/16/2011 - Duo M., Heero Y. - Complete\""
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "meta = re.compile('([a-z+]+) - (.+?) - (?:(.+?) - )?Chapters: ([0-9,]+) - Words: ([0-9,]+)(?: - Reviews: ([0-9,]+))?(?: - Favs: ([0-9,]+))?(?: - Follows: ([0-9,]+))?(?: - Updated: ([0-9/]+))?(?: - Published: ([0-9/]+))?(?: - \\[?(.+?)\\]?)? - Complete', re.IGNORECASE)\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print re.findall(meta, text)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('T', 'English', '', '1', '1,262', '17', '44', '1', '', '10/16/2011', 'Duo M., Heero Y.')]\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Archive of Our Own data scrape\n",
      "\n",
      "* Each data point lives within a single element\n",
      "    * This is great!\n",
      "* But if the data point doesn't exist, the element is omitted.\n",
      "    * This sucks!\n",
      "* I'm not good enough at LXML to handle this elegantly.\n",
      "* So I used a combination of string splitting and regular expressions.\n",
      "    * I know.\n",
      "* Each story was contained between `<li>` elements appearing like:\n",
      "    * `<li class=\"work blurb group\" id=\"work_1877970\" role=\"article\">`\n",
      "    * So I split on `<li class=\"work blurb group\"` and took [1:]\n",
      "        * I know...\n",
      "* Then I looped through these story chunks and extracted stuff from there.\n",
      "    * Only I couldn't get the chunk to play nicely with LXML, so I did regex again."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "storyidre = re.compile(r'id=\"work_([0-9]+)\" role=\"article\">')\n",
      "titlere = re.compile(r'<a href=\"/works/[0-9]+\">(.+?)</a>\\n\\s+by')\n",
      "authorre = re.compile(r'rel=\"author\">(.+?)</a>')\n",
      "descriptionre = re.compile(r'<!--summary-->([\\s\\S]+?)<!--stats-->')\n",
      "languagere = re.compile(r'<dt>Language:</dt>\\n\\s+?<dd>(.+?)</dd>')\n",
      "wordsre = re.compile(r'<dt>Words:</dt>\\n\\s+?<dd>(.+?)</dd>')\n",
      "chaptersre = re.compile(r'<dt>Chapters:</dt>\\n\\s+?<dd>([0-9]+?)/([0-9]+|\\?)</dd>')\n",
      "commentsre = re.compile(r'Comments:</dt>\\n\\s+<dd><a href=\"/works/[0-9]+\\?show_comments=true(?:&amp;view_full_work=true)?(?:#comments)?\">([0-9,]+)</a>')\n",
      "kudosre = re.compile(r'<dt>Kudos:</dt>\\n\\s+<dd><a href=\"/works/[0-9]+(?:#comments|\\?view_full_work=true#comments)\">([0-9,]+)</a>')\n",
      "hitsre = re.compile(r'hits:</dt>\\n\\s+<dd>([0-9,]+)</dd>')\n",
      "ratingre = re.compile(r'<span class=\"rating-.+?\"\\s+title=\".+?\"><span class=\"text\">(.+?)</span>')\n",
      "categoryre = re.compile(r'<span class=\"category-.+?\"\\s+title=\".+?\"><span class=\"text\">(.+?)</span>')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### What I couldn't figure out about LXML\n",
      "\n",
      "* Can I pass it just a string?  How?\n",
      "* The only way I was able to get around this problem was to write out the text to a scratch file and then read it back in.\n",
      "    * This is 100% nuts, but I couldn't get it to work without."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('scratch.txt','rt') as abpage:\n",
      "    body = abpage.read()\n",
      "    page = body.decode('utf-8')\n",
      "parser = etree.HTMLParser()\n",
      "\n",
      "lastxpath = \"//a[text() = 'Last']/@href\"\n",
      "\n",
      "tree = etree.parse(BytesIO(page.encode('utf-8')), parser)\n",
      "last = tree.xpath(lastxpath)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "How can I make `etree.parse()` work while passing a string object?  I tried everything from the documentation and none of it worked."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Data processing\n",
      "\n",
      "All the extracted metadata was dumped into CSV files.  The AO3 files were not a problem as far as size, but the FFnet franchises with more than 50k entries were giving me memory problems.  Metadata in these cases was extracted into several CSV files that were stitched together to create a large CSV file with all the FF net data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "All FFnet metadata scraping was completed just before this presentation was prepared, so the AO3 metadata scraping has not been automated.  As it stands, 2.5 million records were downloaded from FFnet.  These data still need to be cleaned up and have sanity evaluations."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Work is underway to download the full text values for stories from specific fandoms.  After this, the automated AO3 scrape will be taken up."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Full text download\n",
      "\n",
      "## Fanfiction.net\n",
      "\n",
      "FFnet has no link I could find to download the entire story.  Story URLs break down as:\n",
      "\n",
      "* `https://www.fanfiction.net/s/10913118/1/Naruto-Next-Generation`\n",
      "* `/[s for story]/[story URI]/[chapter number]/[simple version of title]'\n",
      "\n",
      "The title portion and chapter number are optional."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Method of downloading:\n",
      "\n",
      "* The story URI and chapter numbers are being gathered in the metadata proces.\n",
      "    * Therefore the metadata for all the stories you want to download must be gathered already.\n",
      "* The loop constructs the various story links and downloads those html files.\n",
      "* This means you have to hit their server for every single chapter. \n",
      "    * 50 chapter story?  50 hits.\n",
      "    * for.e.ver.\n",
      "* I played around with the timing for the scrape and found that 20-30 seconds were fine, although I've gotten away with 10 for a while."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## AO3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "While full text scraping is disallowed within the robots.txt, I emailed them to ask for a data dump and was told to just go ahead and scrape.  Their pages are set up to prevent straight forward downloading, but they do have a handy full text download link.\n",
      "\n",
      "Which looks like this:\n",
      "\n",
      "* `http://www.archiveofourown.org/downloads/my/mysoulislost375/1858989/Naruto%20self%20insertworld%20hopping.html`\n",
      "\n",
      "Which works out to:\n",
      "\n",
      "* `http://www.archiveofourown.org/downloads/[first two letters of author's user name]/[author's user (not pen) name]/[story id]/[sanitized version of the story title].html`\n",
      "\n",
      "Given that I already have the  metadata downloaded, these links were not difficult to reconstruct.  This saved me having to hit the page once to get the download link and again to get the download page.  Of the 9,915 total stories, only 445 of my self-generated links failed.  Most of these were due to extra periods in the title.  This method seems to bypass all their cookie restrictions based on explicitness of the story and the author attempting to prevent downloads.\n",
      "\n",
      "I deliberately kept the scraping timing high, to about 30-45 seconds, just to be kind to their servers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Basic descriptives\n",
      "\n",
      "## Word Count"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Total number of works from each media type.\n",
      "\n",
      "<img src = 'numofstories.png'/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Stories from books were the longest.  Plays were the shortest.\n",
      "\n",
      "<img src = 'wordcountbymedia.png'/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Works rated Mature were the longest.  All ages the shortest.\n",
      "\n",
      "<img src = 'wordcountbyrating.png'/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Favorites"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Mature rated fics get the most favorites.  All ages the least.\n",
      "\n",
      "<img src = 'favbyrating.png'/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Books get the most favorites.  Play the least.  Note that this trend matches with the word length.\n",
      "\n",
      "<img src = 'favbymedia.png'/>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}